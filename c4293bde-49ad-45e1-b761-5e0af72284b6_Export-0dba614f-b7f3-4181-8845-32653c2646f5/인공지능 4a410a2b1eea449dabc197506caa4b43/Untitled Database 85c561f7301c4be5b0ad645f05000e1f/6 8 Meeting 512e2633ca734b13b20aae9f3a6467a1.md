# 6/8 Meeting

Date: June 8, 2024 → June 21, 2024

# 박나연

- Audio feature 잘 뽑아내는 모델 잘 불러오기
    - AST, OmniVec 등 읽고싶은 논문 읽어서, 이게 왜 좋은지 정리
    - 깃허브 찾아서 어떻게 불러왔는지 찾아보기
    - 입력 차원 및 출력 차원 확인하기 (print(tensor.shape) 등의 방법으로 확인)
- 불러온 모델 사용하는 코드를 김도훈한테 보내기
- 모델 학습 파이프라인 코드 받으면 코랩에서 열고 학습하기
- 밑에 코드 콘솔창에 입력하면 코랩 안 끊김

```jsx
function ClickConnect(){
    console.log("코랩 연결 끊김 방지"); 
    document.querySelector("colab-toolbar-button#connect").click() 
}
setInterval(ClickConnect, 60 * 1000)
```

- 궁금한거 있으면 시간 상관없이 그냥 물어보기
    - 오류나면 오류 메시지까지 같이 보내기

Ast model 

- Compared to past models, such as CNN, CNN-attention hybrid model(CNN with self-attention on the top), AST, the first convolution-free, purely attention-based model achieved good results on AudioSet, ESC-50, Speech Commands V2.
- Transformer based architecture is better for processing large dataset compared to CNN based architecture.
- AST 오디오 스펙트로그램을 입력으로 받음, 오디오 데이터에 최적화된 모델

1. Purely attention-based model : CNN은 국소적인 패턴 인식에 뛰어나지만, 긴 범위의 context를 캡처하는 데 한계가 있다. 반면 AST는 모든 레이어에서 global context를 효과적으로 캡처한다
2. 다양한 오디오 분류 벤치마크에서 SOTA 성능을 보여준다.

![Untitled](6%208%20Meeting%20512e2633ca734b13b20aae9f3a6467a1/Untitled.png)

![Untitled](6%208%20Meeting%20512e2633ca734b13b20aae9f3a6467a1/Untitled%201.png)

MELD dataset 

- 감정 인식 및 대화 분석을 위해 설계된 멀티모달 데이터셋
- 텍스트, 오디오, 비디오의 세 가지 모달리티로 구성되어 있음
- 대화 수: 약 1,400개, 발화(turn) 수: 약 13,000개
- 각 발화마다 감정 label이 부여되어 있음(happy, sad, neutral, surprise, anger, disgust, fear)
- 일상적인 대화 상황에서의 데이터를 포함

Final Model

- Vanilla Neural Network - 37%
- Disentanglement -
- CLIP -
- transformer-based model with self-distillation for multimodal ERC
- Ensemble
- MAML(Model-Agnostic Meta-Learning)
    - compatible with any model trained with gradient descent
    
    → can’t apply
    

# 박세용

- Text Feature만 사용해서 감정분류가 얼마나 잘 되는지 확인
    - model freeze 시키고, lr=1e-4정도로 작게 설정
- 박나연과 협업
- 슬슬 ppt 및 발표 준비