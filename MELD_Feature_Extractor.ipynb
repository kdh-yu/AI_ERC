{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvQx-uSiysmP"
      },
      "source": [
        "# Artificial Intelligence - Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bN3_ViiGMbHP"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVsx8xVLy7ta"
      },
      "source": [
        "### Import Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gToQ1t3Iy_y0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "objc[36059]: Class AVFFrameReceiver is implemented in both /Users/tt/miniforge3/envs/ds/lib/python3.8/site-packages/av/.dylibs/libavdevice.60.3.100.dylib (0x1465b8760) and /Users/tt/miniforge3/envs/ds/lib/libavdevice.59.7.100.dylib (0x16982c778). One of the two will be used. Which one is undefined.\n",
            "objc[36059]: Class AVFAudioReceiver is implemented in both /Users/tt/miniforge3/envs/ds/lib/python3.8/site-packages/av/.dylibs/libavdevice.60.3.100.dylib (0x1465b87b0) and /Users/tt/miniforge3/envs/ds/lib/libavdevice.59.7.100.dylib (0x16982c7c8). One of the two will be used. Which one is undefined.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import torchaudio\n",
        "from torchsummary import summary\n",
        "\n",
        "\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dm28yk9zzQ-T"
      },
      "source": [
        "### Download Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jL48r4iFvbq",
        "outputId": "a8681068-252c-4469-9a20-3bf7b450e729"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-06-10 07:31:05--  https://web.eecs.umich.edu/~mihalcea/downloads/MELD.Raw.tar.gz\n",
            "Resolving web.eecs.umich.edu (web.eecs.umich.edu)... 141.212.113.214\n",
            "Connecting to web.eecs.umich.edu (web.eecs.umich.edu)|141.212.113.214|:443... connected.\n",
            "WARNING: cannot verify web.eecs.umich.edu's certificate, issued by ‘CN=InCommon RSA Server CA 2,O=Internet2,C=US’:\n",
            "  Unable to locally verify the issuer's authority.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10878146150 (10G) [application/x-gzip]\n",
            "Saving to: ‘MELD.Raw.tar.gz’\n",
            "\n",
            "MELD.Raw.tar.gz     100%[===================>]  10.13G  14.9MB/s    in 11m 47s \n",
            "\n",
            "2024-06-10 07:42:54 (14.7 MB/s) - ‘MELD.Raw.tar.gz’ saved [10878146150/10878146150]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download Dataset\n",
        "!wget https://web.eecs.umich.edu/~mihalcea/downloads/MELD.Raw.tar.gz --no-check-certificate\n",
        "\n",
        "# Unzip Data\n",
        "!tar -xf MELD.Raw.tar.gz\n",
        "\n",
        "# Unzip Train data\n",
        "!tar -xf ./MELD.Raw/train.tar.gz\n",
        "# Unzip Valid data\n",
        "!tar -xf ./MELD.Raw/dev.tar.gz\n",
        "# Unzip Test data\n",
        "!tar -xf ./MELD.Raw/test.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x302d25df0] moov atom not found\n",
            "OpenCV: Couldn't read video stream from file \"./MELD_Data/train/dia125_utt3.mp4\"\n"
          ]
        }
      ],
      "source": [
        "# Remove missing data\n",
        "BASE_DIR = './MELD_Data/'\n",
        "df_train = pd.read_csv(BASE_DIR + 'train.csv')\n",
        "for idx in range(len(df_train)):\n",
        "    try:\n",
        "        filename = BASE_DIR + 'train/' + 'dia' + str(df_train.iloc[idx]['Dialogue_ID']) + '_utt' + str(df_train.iloc[idx]['Utterance_ID']) + '.mp4'\n",
        "        cv2.VideoCapture(filename)\n",
        "        torchaudio.load(filename)\n",
        "    except:\n",
        "        df_train.drop(idx, inplace=True)\n",
        "df_train.to_csv('./train_filtered.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6318"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(df_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "9989"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train_origin = pd.read_csv(BASE_DIR + 'train.csv')\n",
        "len(df_train_origin)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before : 1109\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "OpenCV: Couldn't read video stream from file \"./MELD_Data/valid/dia110_utt7.mp4\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After : 1107\n"
          ]
        }
      ],
      "source": [
        "# Remove missing data\n",
        "BASE_DIR = './MELD_Data/'\n",
        "df_valid = pd.read_csv(BASE_DIR + 'valid.csv')\n",
        "print(f\"Before : {len(df_valid)}\")\n",
        "for idx in range(len(df_valid)):\n",
        "    try:\n",
        "        filename = BASE_DIR + 'valid/' + 'dia' + str(df_valid.iloc[idx]['Dialogue_ID']) + '_utt' + str(df_valid.iloc[idx]['Utterance_ID']) + '.mp4'\n",
        "        cv2.VideoCapture(filename)\n",
        "        torchaudio.load(filename)\n",
        "    except:\n",
        "        df_valid.drop(idx, inplace=True)\n",
        "print(f\"After : {len(df_valid)}\")\n",
        "df_valid.to_csv('./valid_filtered.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before : 2610\n",
            "After : 1266\n"
          ]
        }
      ],
      "source": [
        "# Remove missing data\n",
        "BASE_DIR = './MELD_Data/'\n",
        "df_test = pd.read_csv(BASE_DIR + 'test.csv')\n",
        "print(f\"Before : {len(df_test)}\")\n",
        "for idx in range(len(df_test)):\n",
        "    try:\n",
        "        filename = BASE_DIR + 'test/' + 'dia' + str(df_test.iloc[idx]['Dialogue_ID']) + '_utt' + str(df_test.iloc[idx]['Utterance_ID']) + '.mp4'\n",
        "        cv2.VideoCapture(filename)\n",
        "        torchaudio.load(filename)\n",
        "    except:\n",
        "        df_test.drop(idx, inplace=True)\n",
        "print(f\"After : {len(df_test)}\")\n",
        "df_test.to_csv('./test_filtered.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgffonhrzUA8"
      },
      "source": [
        "### Preprocess and load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "J2G8D4nZF7q2"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "expected string or bytes-like object",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchaudio\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
            "File \u001b[0;32m~/miniforge3/envs/ds/lib/python3.8/site-packages/transformers/__init__.py:26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     28\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[1;32m     29\u001b[0m     _LazyModule,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m     logging,\n\u001b[1;32m     43\u001b[0m )\n\u001b[1;32m     46\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n",
            "File \u001b[0;32m~/miniforge3/envs/ds/lib/python3.8/site-packages/transformers/dependency_versions_check.py:36\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pkg \u001b[38;5;129;01min\u001b[39;00m deps:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pkg \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;66;03m# must be loaded here, or else tqdm check may fail\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_tokenizers_available\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tokenizers_available():\n\u001b[1;32m     39\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# not required, check version only if installed\u001b[39;00m\n",
            "File \u001b[0;32m~/miniforge3/envs/ds/lib/python3.8/site-packages/transformers/utils/__init__.py:30\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     23\u001b[0m     add_code_sample_docstrings,\n\u001b[1;32m     24\u001b[0m     add_end_docstrings,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     replace_return_docstrings,\n\u001b[1;32m     29\u001b[0m )\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneric\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     31\u001b[0m     ContextManagers,\n\u001b[1;32m     32\u001b[0m     ExplicitEnum,\n\u001b[1;32m     33\u001b[0m     ModelOutput,\n\u001b[1;32m     34\u001b[0m     PaddingStrategy,\n\u001b[1;32m     35\u001b[0m     TensorType,\n\u001b[1;32m     36\u001b[0m     cached_property,\n\u001b[1;32m     37\u001b[0m     can_return_loss,\n\u001b[1;32m     38\u001b[0m     expand_dims,\n\u001b[1;32m     39\u001b[0m     find_labels,\n\u001b[1;32m     40\u001b[0m     flatten_dict,\n\u001b[1;32m     41\u001b[0m     is_jax_tensor,\n\u001b[1;32m     42\u001b[0m     is_numpy_array,\n\u001b[1;32m     43\u001b[0m     is_tensor,\n\u001b[1;32m     44\u001b[0m     is_tf_tensor,\n\u001b[1;32m     45\u001b[0m     is_torch_device,\n\u001b[1;32m     46\u001b[0m     is_torch_dtype,\n\u001b[1;32m     47\u001b[0m     is_torch_tensor,\n\u001b[1;32m     48\u001b[0m     reshape,\n\u001b[1;32m     49\u001b[0m     squeeze,\n\u001b[1;32m     50\u001b[0m     strtobool,\n\u001b[1;32m     51\u001b[0m     tensor_size,\n\u001b[1;32m     52\u001b[0m     to_numpy,\n\u001b[1;32m     53\u001b[0m     to_py_obj,\n\u001b[1;32m     54\u001b[0m     transpose,\n\u001b[1;32m     55\u001b[0m     working_or_temp_dir,\n\u001b[1;32m     56\u001b[0m )\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     58\u001b[0m     CLOUDFRONT_DISTRIB_PREFIX,\n\u001b[1;32m     59\u001b[0m     DISABLE_TELEMETRY,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     85\u001b[0m     send_example_telemetry,\n\u001b[1;32m     86\u001b[0m )\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimport_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     88\u001b[0m     ENV_VARS_TRUE_AND_AUTO_VALUES,\n\u001b[1;32m     89\u001b[0m     ENV_VARS_TRUE_VALUES,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m     torch_version,\n\u001b[1;32m    168\u001b[0m )\n",
            "File \u001b[0;32m~/miniforge3/envs/ds/lib/python3.8/site-packages/transformers/utils/generic.py:29\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, ContextManager, List, Tuple\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimport_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_flax_available, is_tf_available, is_torch_available, is_torch_fx_proxy\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_flax_available():\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mjnp\u001b[39;00m\n",
            "File \u001b[0;32m~/miniforge3/envs/ds/lib/python3.8/site-packages/transformers/utils/import_utils.py:399\u001b[0m\n\u001b[1;32m    397\u001b[0m _torch_fx_available \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _torch_available:\n\u001b[0;32m--> 399\u001b[0m     torch_version \u001b[38;5;241m=\u001b[39m \u001b[43mversion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimportlib_metadata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    400\u001b[0m     _torch_fx_available \u001b[38;5;241m=\u001b[39m (torch_version\u001b[38;5;241m.\u001b[39mmajor, torch_version\u001b[38;5;241m.\u001b[39mminor) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    401\u001b[0m         TORCH_FX_REQUIRED_VERSION\u001b[38;5;241m.\u001b[39mmajor,\n\u001b[1;32m    402\u001b[0m         TORCH_FX_REQUIRED_VERSION\u001b[38;5;241m.\u001b[39mminor,\n\u001b[1;32m    403\u001b[0m     )\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_torch_fx_available\u001b[39m():\n",
            "File \u001b[0;32m~/miniforge3/envs/ds/lib/python3.8/site-packages/packaging/version.py:54\u001b[0m, in \u001b[0;36mparse\u001b[0;34m(version)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(version: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVersion\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     46\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Parse the given version string.\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03m    >>> parse('1.0.dev1')\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m    :raises InvalidVersion: When the version string is not a valid version.\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVersion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mversion\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniforge3/envs/ds/lib/python3.8/site-packages/packaging/version.py:198\u001b[0m, in \u001b[0;36mVersion.__init__\u001b[0;34m(self, version)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Initialize a Version object.\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03m:param version:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m    exception will be raised.\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# Validate the version and parse it into pieces\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m match \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_regex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mversion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m match:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidVersion(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid version: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mversion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
          ]
        }
      ],
      "source": [
        "# DataLoader\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchaudio\n",
        "import torch.nn as nn\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class MELDDataset(Dataset):\n",
        "    def __init__(self, csv, path, transform=None, max_video_len=30, max_audio_len=16000, max_text_len=128):\n",
        "        self.df = pd.read_csv(csv)\n",
        "        self.label = {self.df['Emotion'].unique()[i] : i for i in range(len(self.df['Emotion'].unique()))}\n",
        "        self.path = path\n",
        "        self.max_video_len = max_video_len\n",
        "        self.max_audio_len = max_audio_len\n",
        "        self.max_text_len = max_text_len\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        self.video_transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def load_video(self, video_path):\n",
        "        frames = []\n",
        "        try:\n",
        "            cap = cv2.VideoCapture(video_path)\n",
        "            while cap.isOpened():\n",
        "                ret, frame = cap.read()\n",
        "                if not ret:\n",
        "                    break\n",
        "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                frame = self.video_transform(frame)\n",
        "                frames.append(frame)\n",
        "                if len(frames) >= self.max_video_len:\n",
        "                    break\n",
        "            cap.release()\n",
        "            frames = frames[:self.max_video_len]\n",
        "        except:\n",
        "            pass\n",
        "        if len(frames)==0:\n",
        "            frames.extend([torch.zeros((64, 64))] * (self.max_video_len - len(frames)))\n",
        "        elif len(frames) < self.max_video_len:\n",
        "            frames.extend([torch.zeros_like(frames[0])] * (self.max_video_len - len(frames)))\n",
        "        return torch.stack(frames)\n",
        "\n",
        "    def load_audio(self, audio_path, mel_bins=128, target_length=1024):\n",
        "        try:\n",
        "            waveform, sample_rate = torchaudio.load(audio_path)\n",
        "            fbank = torchaudio.compliance.kaldi.fbank(\n",
        "                waveform, htk_compat=True, sample_frequency=sample_rate, use_energy=False,\n",
        "                window_type='hanning', num_mel_bins=mel_bins, dither=0.0, frame_shift=10)\n",
        "            n_frames = fbank.shape[0]\n",
        "            p = target_length - n_frames\n",
        "            if p > 0:\n",
        "                m = torch.nn.ZeroPad2d((0, 0, 0, p))\n",
        "                fbank = m(fbank)\n",
        "            elif p < 0:\n",
        "                fbank = fbank[0:target_length, :]\n",
        "\n",
        "            fbank = (fbank - (-4.2677393)) / (4.5689974 * 2)\n",
        "            return fbank\n",
        "        except:\n",
        "            return torch.zeros((target_length, mel_bins))\n",
        "\n",
        "    def tokenize_text(self, text):\n",
        "        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_text_len, return_tensors='pt')\n",
        "        return encoding['input_ids'].squeeze(), encoding['attention_mask'].squeeze()\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        filename = 'dia' + str(self.df.iloc[idx]['Dialogue_ID']) + '_utt' + str(self.df.iloc[idx]['Utterance_ID']) + '.mp4'\n",
        "        text = self.df.iloc[idx]['Utterance'].replace('\\x92', \"'\")\n",
        "        video = self.load_video(self.path + filename)\n",
        "        audio = self.load_audio(self.path + filename)\n",
        "        text, attention_mask = self.tokenize_text(text)\n",
        "        label = self.label[self.df.iloc[idx]['Emotion']]\n",
        "        return video, audio, text, attention_mask, label\n",
        "\n",
        "def collate_fn(batch):\n",
        "    videos, audios, texts, attention_masks, labels = zip(*batch)\n",
        "    videos = torch.stack(videos)\n",
        "    audios = torch.stack(audios)\n",
        "    texts = torch.stack(texts)\n",
        "    attention_masks = torch.stack(attention_masks)\n",
        "    labels = torch.tensor(labels)\n",
        "    return videos, audios, texts, attention_masks, labels\n",
        "\n",
        "def MELD(datatype, transform=None, batch_size=2, collate=collate_fn):\n",
        "    \"\"\"DataLoader. \\\\\n",
        "    Expected File structure is: \\\\\n",
        "    ├── train\\\\\n",
        "    ├── valid\\\\\n",
        "    ├── test  \\\\\n",
        "    ├── train.csv\\\\\n",
        "    ├── valid.csv\\\\\n",
        "    └── test.csv\\\\\n",
        "    Change if you want. \\\\\n",
        "    If transform is None, it just resizes data and returns Tensor.\\\\\n",
        "    Video (Batch, Frame, Channel, Height, Width) \\\\\n",
        "    Audio (Batch, Channel, Sample) \\\\\n",
        "    Text  (Batch, tokenized Length)\\\\\n",
        "    Label (Batch)\n",
        "    \"\"\"\n",
        "    # Data to load\n",
        "    if datatype == 'train':\n",
        "        csv_file = '/content/train_sent_emo.csv'\n",
        "        data_folder = '/content/train_splits/'\n",
        "    elif datatype == 'valid':\n",
        "        csv_file = '/content/MELD.Raw/dev_sent_emo.csv'\n",
        "        data_folder = '/content/dev_splits_complete/'\n",
        "    elif datatype == 'test':\n",
        "        csv_file = '/content/MELD.Raw/test_sent_emo.csv'\n",
        "        data_folder = '/content/output_repeated_splits_test/'\n",
        "    # transform\n",
        "    if transform is None:\n",
        "        transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize((64, 64)),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "    # Load data\n",
        "    dataset = MELDDataset(csv_file, data_folder, transform=transform)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "uj3GqGNhXtAr"
      },
      "outputs": [],
      "source": [
        "# DataLoader\n",
        "train_loader = MELD('train')\n",
        "valid_loader = MELD('valid')\n",
        "test_loader = MELD('test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSioBytxYmgH"
      },
      "source": [
        "# Video"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgeJ2LnwzrYE"
      },
      "source": [
        "## Video processing model - Swin3D/B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbcHwIkKz6A8",
        "outputId": "4ecab042-1b8c-4bd7-aaec-0bffb6967ede"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Now using cuda device\n"
          ]
        }
      ],
      "source": [
        "## Select Device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Now using {device} device\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ud4fEic3FlXM",
        "outputId": "398a9b6e-7ec3-4caa-f6ca-f8e93ace8761"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/4995 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "565\n",
            "torch.Size([1024, 128])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/4995 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "145\n",
            "torch.Size([1024, 128])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from torchvision.models.video import swin3d_b\n",
        "train_loader = MELD('train')\n",
        "video_model = swin3d_b(weights='KINETICS400_IMAGENET22K_V1')\n",
        "video_model.head = nn.Identity()\n",
        "video_model = video_model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUdIPxq1XT9T"
      },
      "source": [
        "## Extract Video Feature (Train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjKVzAc7NL7i"
      },
      "outputs": [],
      "source": [
        "features = torch.Tensor([]).to(device)\n",
        "with torch.no_grad():\n",
        "    video_model.eval()\n",
        "    for video, _, _, _, _ in tqdm(train_loader):\n",
        "        video = video.to(device)\n",
        "        video = video.permute(0, 2, 1, 3, 4)\n",
        "        feature = video_model(video)\n",
        "        features = torch.concat([features, feature])\n",
        "torch.save(features, 'video_feature_train.pt')\n",
        "\n",
        "# Download feature tensor\n",
        "files.download('video_feature_train.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esYu1Q-iXmQY"
      },
      "source": [
        "## Extract Video Feature (Valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZgB-xP6Xn_l"
      },
      "outputs": [],
      "source": [
        "features = torch.Tensor([]).to(device)\n",
        "with torch.no_grad():\n",
        "    video_model.eval()\n",
        "    for video, _, _, _, _ in tqdm(valid_loader):\n",
        "        video = video.to(device)\n",
        "        video = video.permute(0, 2, 1, 3, 4)\n",
        "        feature = video_model(video)\n",
        "        features = torch.concat([features, feature])\n",
        "torch.save(features, 'video_feature_valid.pt')\n",
        "\n",
        "# Download feature tensor\n",
        "files.download('video_feature_valid.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FqsHGtNX_oe"
      },
      "source": [
        "## Extract Video Feature (Test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjTngnsYX_LP"
      },
      "outputs": [],
      "source": [
        "features = torch.Tensor([]).to(device)\n",
        "with torch.no_grad():\n",
        "    video_model.eval()\n",
        "    for video, _, _, _, _ in tqdm(test_loader):\n",
        "        video = video.to(device)\n",
        "        video = video.permute(0, 2, 1, 3, 4)\n",
        "        feature = video_model(video)\n",
        "        features = torch.concat([features, feature])\n",
        "torch.save(features, 'video_feature_test.pt')\n",
        "\n",
        "# Download feature tensor\n",
        "files.download('video_feature_test.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnN9q74AYqe9"
      },
      "source": [
        "# Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qS4mlTJA3jRL"
      },
      "source": [
        "## Text processing Model - DeBERTa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 944,
          "referenced_widgets": [
            "91fb39873ff44502bef88832390b7c04",
            "87cc961ea7b840a5bb28d290422a3eb3",
            "d22d0d67afdf415da7c48c0d2ea09ac1",
            "e9517006bf5f4224b576f448c43bc441",
            "f001963998a349c295aa0920c4d14c9c",
            "538bf33863844c3fa961fd18c96f10f7",
            "77948a7060cd4ec2a05a3bd426fa6afa",
            "cf8d738512c14cbfbbe41617882e01f4",
            "be2ef829c9d24607ab8c29a2a2069422",
            "a7a62526be064364bb31cf4096b70945",
            "a5af2caeccb34591af994d2f3410d46f",
            "9fd7ceb40fc44ea59859070bfacdca5d",
            "0113b0327be842cfb96249a618f266a2",
            "b0507c2ccda64f86aff4a943fbf40cd8",
            "49ce94a7720a45059446ab85605763d1",
            "27f33a692c1e4aedb9099bbecf2abab5",
            "fe799a19c68d473bac29a322ca7bd229",
            "eda87d37d83e448ebc6a068190ab4d8c",
            "9d40c4af25394ba0abcc16980392d72a",
            "f4a9b5a9bf6745988179dcc1b7f5e247",
            "e32dedbd03074d458b766b4aa3d7d480",
            "7f4c46be63fa4b728c6b4c9ecb1b836c"
          ]
        },
        "id": "nYhC0Kqo1RXs",
        "outputId": "9331616d-0757-434e-c30c-0cc3aaa6e88a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "91fb39873ff44502bef88832390b7c04",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9fd7ceb40fc44ea59859070bfacdca5d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/371M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AngleSDETextEmbeddingModel(\n",
            "  (deberta): DebertaV2Model(\n",
            "    (embeddings): DebertaV2Embeddings(\n",
            "      (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
            "      (dropout): StableDropout()\n",
            "    )\n",
            "    (encoder): DebertaV2Encoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x DebertaV2Layer(\n",
            "          (attention): DebertaV2Attention(\n",
            "            (self): DisentangledSelfAttention(\n",
            "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (pos_dropout): StableDropout()\n",
            "              (dropout): StableDropout()\n",
            "            )\n",
            "            (output): DebertaV2SelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
            "              (dropout): StableDropout()\n",
            "            )\n",
            "          )\n",
            "          (intermediate): DebertaV2Intermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): DebertaV2Output(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
            "            (dropout): StableDropout()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (rel_embeddings): Embedding(512, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (sde_layer): Linear(in_features=768, out_features=768, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "from transformers import DebertaV2Model\n",
        "\n",
        "class AngleSDETextEmbeddingModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AngleSDETextEmbeddingModel, self).__init__()\n",
        "        self.deberta = DebertaV2Model.from_pretrained('microsoft/deberta-v3-base')\n",
        "        self.sde_layer = nn.Linear(768, 768)\n",
        "        self.angle_weight = nn.Parameter(torch.Tensor(768, 768))\n",
        "        nn.init.uniform_(self.angle_weight)\n",
        "\n",
        "    def angle_optimization(self, embeddings):\n",
        "        norm = embeddings.norm(p=2, dim=1, keepdim=True)\n",
        "        normalized_embeddings = embeddings / norm\n",
        "        angles = torch.mm(normalized_embeddings, self.angle_weight)\n",
        "        return angles\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.deberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        last_hidden_state = outputs.last_hidden_state\n",
        "        pooled_output = last_hidden_state[:, 0, :]\n",
        "\n",
        "        # Apply SDE\n",
        "        sde_output = self.sde_layer(pooled_output)\n",
        "        sde_output = F.relu(sde_output)\n",
        "\n",
        "        # Apply AnglE\n",
        "        angle_output = self.angle_optimization(sde_output)\n",
        "\n",
        "        return angle_output\n",
        "\n",
        "text_model = AngleSDETextEmbeddingModel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDNXbTqZYRF0"
      },
      "source": [
        "## Extract Text Feature (Train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uu_Mg6FLAA-"
      },
      "outputs": [],
      "source": [
        "text_model = text_model.to(device)\n",
        "features = torch.Tensor([]).to(device)\n",
        "with torch.no_grad():\n",
        "    text_model.eval()\n",
        "    for _, _, text, attn, _ in tqdm(train_loader):\n",
        "        text, attn = text.to(device), attn.to(device)\n",
        "        feature = text_model(text, attn)\n",
        "        features = torch.concat([features, feature])\n",
        "torch.save(features, 'text_feature_train.pt')\n",
        "files.download('text_feature_train.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ikGj-DuYWS9"
      },
      "source": [
        "## Extract Text Feature (Valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRRgxB6JYVMY"
      },
      "outputs": [],
      "source": [
        "text_model = text_model.to(device)\n",
        "features = torch.Tensor([]).to(device)\n",
        "with torch.no_grad():\n",
        "    text_model.eval()\n",
        "    for _, _, text, attn, _ in tqdm(valid_loader):\n",
        "        text, attn = text.to(device), attn.to(device)\n",
        "        feature = text_model(text, attn)\n",
        "        features = torch.concat([features, feature])\n",
        "torch.save(features, 'text_feature_valid.pt')\n",
        "files.download('text_feature_valid.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXuYaeqMYd1k"
      },
      "source": [
        "## Extract Text Feature (Test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TbNHisPYVsn"
      },
      "outputs": [],
      "source": [
        "text_model = text_model.to(device)\n",
        "features = torch.Tensor([]).to(device)\n",
        "with torch.no_grad():\n",
        "    text_model.eval()\n",
        "    for _, _, text, attn, _ in tqdm(test_loader):\n",
        "        text, attn = text.to(device), attn.to(device)\n",
        "        feature = text_model(text, attn)\n",
        "        features = torch.concat([features, feature])\n",
        "torch.save(features, 'text_feature_test.pt')\n",
        "files.download('text_feature_test.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTvf08PSYtP6"
      },
      "source": [
        "# Audio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEVDW88uM2mx"
      },
      "source": [
        "## Audio processing model: AST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYmhXbnwKIZM",
        "outputId": "fcc57543-5dfb-42cd-a8e4-33e91772f67c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'ast'...\n",
            "remote: Enumerating objects: 649, done.\u001b[K\n",
            "remote: Counting objects: 100% (209/209), done.\u001b[K\n",
            "remote: Compressing objects: 100% (50/50), done.\u001b[K\n",
            "remote: Total 649 (delta 172), reused 159 (delta 159), pack-reused 440\u001b[K\n",
            "Receiving objects: 100% (649/649), 2.41 MiB | 24.89 MiB/s, done.\n",
            "Resolving deltas: 100% (360/360), done.\n",
            "/content/ast\n",
            "Collecting timm==0.4.5\n",
            "  Downloading timm-0.4.5-py3-none-any.whl (287 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.4/287.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.10/dist-packages (from timm==0.4.5) (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm==0.4.5) (0.18.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.5) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.5) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.5) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.5) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.5) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.5) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.4->timm==0.4.5)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.4->timm==0.4.5)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.4->timm==0.4.5)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.4->timm==0.4.5)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.4->timm==0.4.5)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.4->timm==0.4.5)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.4->timm==0.4.5)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.4->timm==0.4.5)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.4->timm==0.4.5)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.4->timm==0.4.5)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.4->timm==0.4.5)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.5) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.4->timm==0.4.5)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm==0.4.5) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm==0.4.5) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4->timm==0.4.5) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4->timm==0.4.5) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, timm\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 timm-0.4.5\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9656 sha256=5005fb79b0a87a49471f44a90df385c50855803a7c15dcd96b80afe35c2f83bb\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ],
      "source": [
        "# Initial Settings\n",
        "import sys\n",
        "!git clone https://github.com/YuanGongND/ast\n",
        "sys.path.append('./ast')\n",
        "%cd /content/ast/\n",
        "!pip install timm==0.4.5\n",
        "!pip install wget\n",
        "import os, csv, argparse, wget\n",
        "os.environ['TORCH_HOME'] = '/content/ast/pretrained_models'\n",
        "if os.path.exists('/content/ast/pretrained_models') == False:\n",
        "    os.mkdir('/content/ast/pretrained_models')\n",
        "import torch, torchaudio, timm\n",
        "import numpy as np\n",
        "from torch.cuda.amp import autocast\n",
        "import IPython\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZN-_wLvKVwu"
      },
      "outputs": [],
      "source": [
        "# Model Definition\n",
        "from src.models import ASTModel\n",
        "class ASTModelVis(ASTModel):\n",
        "    def get_att_map(self, block, x):\n",
        "        qkv = block.attn.qkv\n",
        "        num_heads = block.attn.num_heads\n",
        "        scale = block.attn.scale\n",
        "        B, N, C = x.shape\n",
        "        qkv = qkv(x).reshape(B, N, 3, num_heads, C // num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
        "        attn = (q @ k.transpose(-2, -1)) * scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        return attn\n",
        "\n",
        "    def forward_visualization(self, x):\n",
        "        # expect input x = (batch_size, time_frame_num, frequency_bins), e.g., (12, 1024, 128)\n",
        "        x = x.unsqueeze(1)\n",
        "        x = x.transpose(2, 3)\n",
        "\n",
        "        B = x.shape[0]\n",
        "        x = self.v.patch_embed(x)\n",
        "        cls_tokens = self.v.cls_token.expand(B, -1, -1)\n",
        "        dist_token = self.v.dist_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, dist_token, x), dim=1)\n",
        "        x = x + self.v.pos_embed\n",
        "        x = self.v.pos_drop(x)\n",
        "        # save the attention map of each of 12 Transformer layer\n",
        "        att_list = []\n",
        "        for blk in self.v.blocks:\n",
        "            cur_att = self.get_att_map(blk, x)\n",
        "            att_list.append(cur_att)\n",
        "            x = blk(x)\n",
        "        return att_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTHL4AmbOTVs",
        "outputId": "4e14a1b4-9499-47da-ddb4-ffd2d1722e46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------AST Model Summary---------------\n",
            "ImageNet pretraining: False, AudioSet pretraining: False\n",
            "frequncey stride=10, time stride=10\n",
            "number of patches=1212\n",
            "DataParallel(\n",
            "  (module): ASTModelVis(\n",
            "    (v): DistilledVisionTransformer(\n",
            "      (patch_embed): PatchEmbed(\n",
            "        (proj): Conv2d(1, 768, kernel_size=(16, 16), stride=(10, 10))\n",
            "      )\n",
            "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
            "      (blocks): ModuleList(\n",
            "        (0-11): 12 x Block(\n",
            "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (attn): Attention(\n",
            "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (drop_path): Identity()\n",
            "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): Mlp(\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (act): GELU(approximate='none')\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (drop): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (pre_logits): Identity()\n",
            "      (head): Linear(in_features=768, out_features=1000, bias=True)\n",
            "      (head_dist): Linear(in_features=768, out_features=1000, bias=True)\n",
            "    )\n",
            "    (mlp_head): Sequential(\n",
            "      (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (1): Identity()\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Model Initiation and load pre-trained weights\n",
        "audioset_mdl_url = 'https://www.dropbox.com/s/cv4knew8mvbrnvq/audioset_0.4593.pth?dl=1'\n",
        "if os.path.exists('/content/ast/pretrained_models/audio_mdl.pth') == False:\n",
        "    wget.download(audioset_mdl_url, out='/content/ast/pretrained_models/audio_mdl.pth')\n",
        "input_tdim = 1024\n",
        "ast_mdl = ASTModelVis(label_dim=527, input_tdim=input_tdim, imagenet_pretrain=False, audioset_pretrain=False)\n",
        "checkpoint_path = '/content/ast/pretrained_models/audio_mdl.pth'\n",
        "checkpoint = torch.load(checkpoint_path, map_location='cuda')\n",
        "audio_model = torch.nn.DataParallel(ast_mdl, device_ids=[0])\n",
        "audio_model.load_state_dict(checkpoint)\n",
        "audio_model.module.mlp_head[1] = nn.Identity()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqRKPG7KYxQZ"
      },
      "source": [
        "## Extract Audio Feature (Train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6m6ssu0YLqND"
      },
      "outputs": [],
      "source": [
        "audio_model = audio_model.to(device)\n",
        "features = torch.Tensor([]).to(device)\n",
        "with torch.no_grad():\n",
        "    audio_model.eval()\n",
        "    for _, audio, _, _, _ in tqdm(train_loader):\n",
        "        audio = audio.to(device)\n",
        "        feature = audio_model(audio)\n",
        "        features = torch.concat([features, feature])\n",
        "torch.save(features, 'audio_feature_train.pt')\n",
        "files.download('audio_feature_train.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHajR3gAZMuF"
      },
      "source": [
        "## Extract Audio Feature (Valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-QboN4qZLz7"
      },
      "outputs": [],
      "source": [
        "audio_model = audio_model.to(device)\n",
        "features = torch.Tensor([]).to(device)\n",
        "with torch.no_grad():\n",
        "    audio_model.eval()\n",
        "    for _, audio, _, _, _ in tqdm(valid_loader):\n",
        "        audio = audio.to(device)\n",
        "        feature = audio_model(audio)\n",
        "        features = torch.concat([features, feature])\n",
        "torch.save(features, 'audio_feature_valid.pt')\n",
        "files.download('audio_feature_valid.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-Shz8z0ZXLP"
      },
      "source": [
        "## Extract Audio Feature (Test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydX5-yV0ZMKR"
      },
      "outputs": [],
      "source": [
        "audio_model = audio_model.to(device)\n",
        "features = torch.Tensor([]).to(device)\n",
        "with torch.no_grad():\n",
        "    audio_model.eval()\n",
        "    for _, audio, _, _, _ in tqdm(test_loader):\n",
        "        audio = audio.to(device)\n",
        "        feature = audio_model(audio)\n",
        "        features = torch.concat([features, feature])\n",
        "torch.save(features, 'audio_feature_test.pt')\n",
        "files.download('audio_feature_test.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q79bENMl3-w5"
      },
      "source": [
        "# Final Classifying model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOe6aLZqFjPH"
      },
      "outputs": [],
      "source": [
        "# DataLoader\n",
        "train_loader = MELD('train')\n",
        "valid_loader = MELD('valid')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1enEPFTB0wMM"
      },
      "outputs": [],
      "source": [
        "###################\n",
        "# Vanilla Version #\n",
        "###################\n",
        "class MELDClassifier(nn.Module):\n",
        "    def __init__(self, video_model, audio_model, text_model):\n",
        "        # Feature Extractor\n",
        "        super().__init__()\n",
        "        self.video_model = video_model\n",
        "        self.audio_model = audio_model\n",
        "        self.text_model = text_model\n",
        "        self.video_mapping = nn.Linear(1024, 768)\n",
        "        self.audio_mapping = nn.Linear(768, 768)\n",
        "        self.text_mapping = nn.Linear(768, 768)\n",
        "        # Weight\n",
        "        self.Wv = nn.Parameter(torch.Tensor([1]))\n",
        "        self.Wa = nn.Parameter(torch.Tensor([1]))\n",
        "        self.Wt = nn.Parameter(torch.Tensor([1]))\n",
        "        # Classifier\n",
        "        self.clf = nn.Linear(768, 7)\n",
        "\n",
        "    def forward(self, v, a, t, attm):\n",
        "        # Extract video feature and map\n",
        "        fv = self.video_model(v)\n",
        "        fv = self.video_mapping(fv)\n",
        "        # Extract audio feature and map\n",
        "        fa = self.audio_model(a)\n",
        "        fa = self.audio_mapping(fa)\n",
        "        # Extract text feature and map\n",
        "        ft = self.text_model(t, attm)\n",
        "        ft = self.text_mapping(ft)\n",
        "        # Option 1: just weighting them\n",
        "        feature = self.Wv * fv + self.Wa * fa + self.Wt * ft\n",
        "        output = self.clf(feature)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvgpdeyKCRnz"
      },
      "outputs": [],
      "source": [
        "# Model\n",
        "model = MELDClassifier(video_model=video_model,\n",
        "                       audio_model=audio_model,\n",
        "                       text_model=text_model).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "model.train()\n",
        "# Train\n",
        "for epoch in range(10):\n",
        "    loss_tmp = 0\n",
        "    acc_tmp = 0\n",
        "    for video, audio, text, attm, label in tqdm(train_loader):\n",
        "        video, audio, text, attm, label = video.to(device), audio.to(device), text.to(device), attm.to(device), label.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        video = video.permute(0, 2, 1, 3, 4)\n",
        "        yhat = model(video, audio, text, attm)\n",
        "        loss = loss_fn(yhat, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_tmp += loss.item()\n",
        "        acc_tmp += (yhat.argmax(1) == label).type(torch.float).sum().item()\n",
        "    print(f\"Epoch {epoch} : Accuracy {acc_tmp:.2f}, Loss {loss_tmp:.2f}\")\n",
        "    torch.save(model.state_dict(), f'model_vanilla_epoch_{epoch}.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZ2txXtXGanl"
      },
      "outputs": [],
      "source": [
        "# Validation\n",
        "load_existing_pt = -1  # Indicate specific epoch\n",
        "if load_existing_pt >= 1:\n",
        "    model.load_state_dict(torch.load(f'/content/model_vanilla_epoch_{str(load_existing_pt)}.pt'))\n",
        "\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    loss_tmp = 0\n",
        "    acc_tmp = 0\n",
        "    for video, audio, text, attm, label in tqdm(valid_loader):\n",
        "        video, audio, text, attm, label = video.to(device), audio.to(device), text.to(device), attm.to(device), label.to(device)\n",
        "        video = video.permute(0, 2, 1, 3, 4)\n",
        "        yhat = model(video, audio, text, attm)\n",
        "        loss = loss_fn(yhat, label)\n",
        "        loss_tmp += loss.item()\n",
        "        acc_tmp += (yhat.argmax(1) == label).type(torch.float).sum().item()\n",
        "    print(f\"Accuracy {acc_tmp:.2f}, Loss {loss_tmp:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cK5CoCcMzV6W"
      },
      "outputs": [],
      "source": [
        "###################\n",
        "# DDM+CFM Version #\n",
        "###################\n",
        "class MELDClassifier(nn.Module):\n",
        "    def __init__(self, video_model, audio_model, text_model):\n",
        "        # Feature Extractor\n",
        "        super().__init__()\n",
        "        self.video_model = video_model\n",
        "        self.audio_model = audio_model\n",
        "        self.text_model = text_model\n",
        "        # Modality and Utterance encoder\n",
        "        self.video_projection = nn.Linear(1024, 768)\n",
        "        self.video_me = nn.Linear(768, 768)\n",
        "        self.video_ue = nn.Linear(768, 768)\n",
        "        self.audio_me = nn.Linear(768, 768)\n",
        "        self.audio_ue = nn.Linear(768, 768)\n",
        "        self.text_me = nn.Linear(768, 768)\n",
        "        self.text_ue = nn.Linear(768, 768)\n",
        "        # Weight\n",
        "        self.Wv = nn.Linear(768*3, 1)\n",
        "        self.Wa = nn.Linear(768*3, 1)\n",
        "        self.Wt = nn.Linear(768*3, 1)\n",
        "        # Auxilary Loss\n",
        "        self.mse = nn.MSELoss()\n",
        "        # TCP\n",
        "        self.TCPv = nn.Linear(768*3, 7)\n",
        "        self.TCPa = nn.Linear(768*3, 7)\n",
        "        self.TCPt = nn.Linear(768*3, 7)\n",
        "        # Classifier\n",
        "        self.clf = nn.Linear(768*3, 7)\n",
        "\n",
        "    def forward(self, v, a, t, attm):\n",
        "        loss = 0\n",
        "        # Extract video feature, map and encode\n",
        "        fv = self.video_model(v)\n",
        "        fv = self.video_projection(fv)\n",
        "        fv_m = self.video_me(fv)\n",
        "        fv_u = self.video_ue(fv)\n",
        "        # Extract audio feature and encode\n",
        "        fa = self.audio_model(a)\n",
        "        fa_m = self.audio_me(fa)\n",
        "        fa_u = self.audio_ue(fa)\n",
        "        # Extract text feature and encode\n",
        "        ft = self.text_model(t, attm)\n",
        "        ft_m = self.text_me(ft)\n",
        "        ft_u = self.text_ue(ft)\n",
        "\n",
        "        ############ DDM ############\n",
        "        if self.training:\n",
        "            # Contrastive Learning: Prepare\n",
        "            B = ft.shape[0]\n",
        "            f_modality = torch.empty(B, 768*3)\n",
        "            f_modality[0::3] = fv_m\n",
        "            f_modality[1::3] = fa_m\n",
        "            f_modality[2::3] = ft_m\n",
        "            f_utterance = torch.empty(B, 768*3)\n",
        "            f_utterance[0::3] = fv_u\n",
        "            f_utterance[1::3] = fa_u\n",
        "            f_utterance[2::3] = ft_u\n",
        "            # Contrastive Learning: Modality\n",
        "            cos_sim = F.cosine_similarity(f_modality.unsqueeze(1), f_modality.unsqueeze(0), dim=-1)\n",
        "            pos_indices = torch.arange(0, B*3).reshape(3, B).T\n",
        "            pos_loss = 0\n",
        "            for i in range(3):\n",
        "                loss += (1 - cos_sim[pos_indices[:, i], pos_indices[:, i]]).mean()\n",
        "            for i in range(3):\n",
        "                for j in range(i + 1, 3):\n",
        "                    loss += cos_sim[pos_indices[:, i], pos_indices[:, j]].mean()\n",
        "            # Contrastive Learning: Utterance\n",
        "            cos_sim = F.cosine_similarity(f_utterance.unsqueeze(1), f_utterance.unsqueeze(0), dim=-1)\n",
        "            loss = torch.mean(torch.triu(cos_sim, diagonal=1))\n",
        "        # Concat each feature vectors\n",
        "        fv = torch.concat([fv, fv_m, fv_u])\n",
        "        fa = torch.concat([fa, fa_m, fa_u])\n",
        "        ft = torch.concat([ft, ft_m, ft_u])\n",
        "\n",
        "        ############ CFM ############\n",
        "        Wv = torch.sigmoid(self.Wv(fv))\n",
        "        Wa = torch.sigmoid(self.Wa(fa))\n",
        "        Wt = torch.sigmoid(self.Wt(ft))\n",
        "        if self.training:\n",
        "            # TCP, True Classification Probability\n",
        "            logit_v = self.TCPv(fv)\n",
        "            logit_a = self.TCPa(fa)\n",
        "            logit_t = self.TCPt(ft)\n",
        "            # TCP Loss\n",
        "            Lv = F.softmax(logit_v).max(1)[0]\n",
        "            La = F.softmax(logit_a).max(1)[0]\n",
        "            Lt = F.softmax(logit_t).max(1)[0]\n",
        "            loss += self.mse(Lv, Wv)\n",
        "            loss += self.mse(La, Wa)\n",
        "            loss += self.mse(Lt, Wt)\n",
        "\n",
        "        # Weight\n",
        "        h = Wv*fv + Wa*fa + Wt*ft\n",
        "\n",
        "        ############ Classifier ############\n",
        "        output = self.clf(h)\n",
        "        if self.training:\n",
        "            return output, loss, logit_v, logit_a, logit_t\n",
        "        else:\n",
        "            return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FTX9HFrFnw9"
      },
      "outputs": [],
      "source": [
        "# Model\n",
        "model = MELDClassifier(video_model=video_model,\n",
        "                       audio_model=audio_model,\n",
        "                       text_model=text_model).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "# Train\n",
        "model.train()\n",
        "for epoch in range(10):\n",
        "    loss_tmp = 0\n",
        "    acc_tmp = 0\n",
        "    for video, audio, text, attm, label in tqdm(train_loader):\n",
        "        video, audio, text, attm, label = video.to(device), audio.to(device), text.to(device), attm.to(device), label.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        video = video.permute(0, 2, 1, 3, 4)\n",
        "        yhat, loss, lv, la, lt = model(video, audio, text, attm)\n",
        "        loss += loss_fn(yhat, label)\n",
        "        loss += loss_fn(yhat, lv)\n",
        "        loss += loss_fn(yhat, la)\n",
        "        loss += loss_fn(yhat, lt)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_tmp += loss.item()\n",
        "        acc_tmp += (yhat.argmax(1) == label).type(torch.float).sum().item()\n",
        "    print(f\"Epoch {epoch} : Accuracy {acc_tmp:.2f}, Loss {loss_tmp:.2f}\")\n",
        "    torch.save(model.state_dict(), f'model_epoch_{epoch}.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4q7eTFNHddu"
      },
      "outputs": [],
      "source": [
        "# Validation\n",
        "load_existing_pt = -1  # Indicate specific epoch\n",
        "if load_existing_pt >= 1:\n",
        "    model.load_state_dict(torch.load(f'/content/model_epoch_{str(load_existing_pt)}.pt'))\n",
        "\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    loss_tmp = 0\n",
        "    acc_tmp = 0\n",
        "    for video, audio, text, attm, label in tqdm(valid_loader):\n",
        "        video, audio, text, attm, label = video.to(device), audio.to(device), text.to(device), attm.to(device), label.to(device)\n",
        "        video = video.permute(0, 2, 1, 3, 4)\n",
        "        yhat = model(video, audio, text, attm)\n",
        "        loss = loss_fn(yhat, label)\n",
        "        loss_tmp += loss.item()\n",
        "        acc_tmp += (yhat.argmax(1) == label).type(torch.float).sum().item()\n",
        "    print(f\"Accuracy {acc_tmp:.2f}, Loss {loss_tmp:.2f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "bN3_ViiGMbHP",
        "CVsx8xVLy7ta",
        "vSioBytxYmgH",
        "qgeJ2LnwzrYE",
        "gnN9q74AYqe9",
        "qS4mlTJA3jRL",
        "VTvf08PSYtP6",
        "mEVDW88uM2mx",
        "Q79bENMl3-w5"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0113b0327be842cfb96249a618f266a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe799a19c68d473bac29a322ca7bd229",
            "placeholder": "​",
            "style": "IPY_MODEL_eda87d37d83e448ebc6a068190ab4d8c",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "27f33a692c1e4aedb9099bbecf2abab5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49ce94a7720a45059446ab85605763d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e32dedbd03074d458b766b4aa3d7d480",
            "placeholder": "​",
            "style": "IPY_MODEL_7f4c46be63fa4b728c6b4c9ecb1b836c",
            "value": " 371M/371M [00:01&lt;00:00, 260MB/s]"
          }
        },
        "538bf33863844c3fa961fd18c96f10f7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77948a7060cd4ec2a05a3bd426fa6afa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7f4c46be63fa4b728c6b4c9ecb1b836c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "87cc961ea7b840a5bb28d290422a3eb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_538bf33863844c3fa961fd18c96f10f7",
            "placeholder": "​",
            "style": "IPY_MODEL_77948a7060cd4ec2a05a3bd426fa6afa",
            "value": "config.json: 100%"
          }
        },
        "91fb39873ff44502bef88832390b7c04": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_87cc961ea7b840a5bb28d290422a3eb3",
              "IPY_MODEL_d22d0d67afdf415da7c48c0d2ea09ac1",
              "IPY_MODEL_e9517006bf5f4224b576f448c43bc441"
            ],
            "layout": "IPY_MODEL_f001963998a349c295aa0920c4d14c9c"
          }
        },
        "9d40c4af25394ba0abcc16980392d72a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9fd7ceb40fc44ea59859070bfacdca5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0113b0327be842cfb96249a618f266a2",
              "IPY_MODEL_b0507c2ccda64f86aff4a943fbf40cd8",
              "IPY_MODEL_49ce94a7720a45059446ab85605763d1"
            ],
            "layout": "IPY_MODEL_27f33a692c1e4aedb9099bbecf2abab5"
          }
        },
        "a5af2caeccb34591af994d2f3410d46f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a7a62526be064364bb31cf4096b70945": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0507c2ccda64f86aff4a943fbf40cd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d40c4af25394ba0abcc16980392d72a",
            "max": 371146213,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f4a9b5a9bf6745988179dcc1b7f5e247",
            "value": 371146213
          }
        },
        "be2ef829c9d24607ab8c29a2a2069422": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cf8d738512c14cbfbbe41617882e01f4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d22d0d67afdf415da7c48c0d2ea09ac1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf8d738512c14cbfbbe41617882e01f4",
            "max": 579,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_be2ef829c9d24607ab8c29a2a2069422",
            "value": 579
          }
        },
        "e32dedbd03074d458b766b4aa3d7d480": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9517006bf5f4224b576f448c43bc441": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7a62526be064364bb31cf4096b70945",
            "placeholder": "​",
            "style": "IPY_MODEL_a5af2caeccb34591af994d2f3410d46f",
            "value": " 579/579 [00:00&lt;00:00, 25.8kB/s]"
          }
        },
        "eda87d37d83e448ebc6a068190ab4d8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f001963998a349c295aa0920c4d14c9c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4a9b5a9bf6745988179dcc1b7f5e247": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fe799a19c68d473bac29a322ca7bd229": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
